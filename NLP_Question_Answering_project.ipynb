{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c35d43e1",
   "metadata": {},
   "source": [
    "# Install and Import Lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a19c5a",
   "metadata": {},
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d733545c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adnane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\adnane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "import textwrap\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4c917a",
   "metadata": {},
   "source": [
    "# Prétraitement du text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "186a59cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction pour faire la tokenisation\n",
    "def tok_func(data):\n",
    "  data = nltk.word_tokenize(data)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d9ba7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction pour la suppression des stopWords\n",
    "def stpword_func(data):\n",
    "  stopwords_en = set(stopwords.words('english'))\n",
    "  data = [w for w in data if w not in stopwords_en]\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56d473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction pour supprimer la ponctuation\n",
    "def ponc_fonc(data):\n",
    "  ponc = string.punctuation\n",
    "  data = [w for w in data if w not in ponc]\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4518bf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction pour transformer le text en minuscule\n",
    "def min_fonc(data):\n",
    "  data = [w.lower() for w in data]\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4da7e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction pour faire steaming\n",
    "def stem_func(data):\n",
    "  stem = PorterStemmer()\n",
    "  data = [stem.stem(w) for w in data]\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5a34e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction pour clean_text\n",
    "def clean_func(data):\n",
    "  data = tok_func(data)\n",
    "  data = stpword_func(data)\n",
    "  data = ponc_fonc(data)\n",
    "  data = min_fonc(data)\n",
    "  data = stem_func(data)\n",
    "  data = \" \".join(data)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6850bb37",
   "metadata": {},
   "source": [
    "# Q&A model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b88d525",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aec29ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50173dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, answer_text):\n",
    "    # Encode la question et le texte de réponse en utilisant le tokenizer\n",
    "    input_ids = tokenizer.encode(question, answer_text)\n",
    "\n",
    "    # Trouve l'index du token de séparation [SEP]\n",
    "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "    # Calcule le nombre de tokens dans la première et deuxième séquence\n",
    "    num_seg_a = sep_index + 1\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "    # Crée les ids de segment pour chaque token\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "    # Vérifie que la longueur des ids de segment est la même que celle des input_ids\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "    # Effectue la prédiction en utilisant le modèle\n",
    "    outputs = model(torch.tensor([input_ids]), \n",
    "                    token_type_ids=torch.tensor([segment_ids]), \n",
    "                    return_dict=True)\n",
    "\n",
    "    # Récupère les scores de début et de fin de réponse\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "\n",
    "    # Trouve l'index du token de début et de fin de réponse avec les scores max\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "\n",
    "    # Convertit les ids de tokens en tokens réels\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    # Concatène les tokens pour former la réponse\n",
    "    answer = tokens[answer_start]\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "        if tokens[i][0:2] == '##':\n",
    "            answer += tokens[i][2:]\n",
    "        else:\n",
    "            answer += ' ' + tokens[i]\n",
    "\n",
    "    # Affiche la réponse\n",
    "    print('Réponse : \"' + answer + '\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6161bf",
   "metadata": {},
   "source": [
    "# Tester le modéle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267143f8",
   "metadata": {},
   "source": [
    "## Answer_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d28925",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper = textwrap.TextWrapper(width=80) \n",
    "bert_abstract = \"Musk was born in Pretoria, South Africa, and briefly attended at the University of Pretoria before moving to Canada at age 18, acquiring citizenship through his Canadian-born mother. Two years later, he matriculated at Queen's University and transferred to the University of Pennsylvania, where he received bachelor's degrees in economics and physics. He moved to California in 1995 to attend Stanford University. After two days, he dropped out and, with his brother Kimbal, co-founded the online city guide software company Zip2. In 1999, Zip2 was acquired by Compaq for $307 million and Musk co-founded X.com, a direct bank. X.com merged with Confinity in 2000 to form PayPal, which eBay acquired for $1.5 billion in 2002. Musk received an EB-5 investor green card in 1997, which led to his U.S. citizenship in 2002.[8]\"\n",
    "bert_abstract = clean_func(bert_abstract)\n",
    "print(wrapper.fill(bert_abstract))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e971367",
   "metadata": {},
   "source": [
    "## Question_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db0bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"where musk is born?\"\n",
    "question = clean_func(question)\n",
    "answer_question(question, bert_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510e25b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
